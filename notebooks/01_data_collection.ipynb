{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Collection\n",
    "\n",
    "This notebook collects electricity consumption data from the EIA API and other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PHASE 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from .env file\n",
      "‚úÖ Setup complete!\n",
      "   üì¶ Pandas: 2.3.3\n",
      "   üì¶ NumPy: 2.0.2\n",
      "   üì¶ Requests: 2.32.5\n",
      "   üîë API Key: eEtxdrdZCI...\n",
      "   üåê Base URL: https://api.eia.gov/v2\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path().parent.parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Load environment variables (with error handling)\n",
    "try:\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load .env file: {e}\")\n",
    "    print(\"   Using default/fallback values\")\n",
    "\n",
    "# Set API key (if required)\n",
    "EIA_API_KEY = os.getenv('EIA_API_KEY', 'eEtxdrdZCIF4DqfXmiVrLdVbRkOK8vcQdfY1ZQOs')\n",
    "\n",
    "# Define base URL\n",
    "BASE_URL = \"https://api.eia.gov/v2\"\n",
    "\n",
    "# Verify setup\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"   üì¶ Pandas: {pd.__version__}\")\n",
    "print(f\"   üì¶ NumPy: {np.__version__}\")\n",
    "print(f\"   üì¶ Requests: {requests.__version__}\")\n",
    "print(f\"   üîë API Key: {EIA_API_KEY[:10]}...\")\n",
    "print(f\"   üåê Base URL: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PHASE 1: Data Ingestion Pipeline\n",
    "\n",
    "## Step 1: Define Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(offset=0, length=5000):\n",
    "    \"\"\"\n",
    "    Fetch retail sales data from EIA API\n",
    "    \n",
    "    Args:\n",
    "        offset: Starting record number (for pagination)\n",
    "        length: Number of records to fetch (max 5000 per request)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DataFrame, total_count, has_more)\n",
    "    \"\"\"\n",
    "    url = \"https://api.eia.gov/v2/electricity/retail-sales/data/?frequency=monthly&data[0]=customers&data[1]=price&data[2]=revenue&data[3]=sales&sort[0][column]=period&sort[0][direction]=desc&offset={}&length={}\"\n",
    "    \n",
    "    # Add API key to URL\n",
    "    url_with_key = url.format(offset, length) + f\"&api_key={EIA_API_KEY}\"\n",
    "    \n",
    "    response = requests.get(url_with_key)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    \n",
    "    json_response = response.json()\n",
    "    data = json_response['response']['data']\n",
    "    total_count = json_response['response'].get('total', len(data))\n",
    "    \n",
    "    # Convert total_count to int if it's a string\n",
    "    try:\n",
    "        total_count = int(total_count) if total_count is not None else len(data)\n",
    "    except (ValueError, TypeError):\n",
    "        total_count = len(data)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Determine if there's more data to fetch\n",
    "    # More data exists if we got a full batch AND haven't reached the total count\n",
    "    has_more = len(data) == length and (offset + length) < total_count\n",
    "    \n",
    "    return df, total_count, has_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_data():\n",
    "    \"\"\"\n",
    "    Fetch all retail sales data from EIA API (handles pagination)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all electricity retail sales data\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Fetching data from EIA API...\")\n",
    "    all_dataframes = []\n",
    "    offset = 0\n",
    "    length = 5000\n",
    "    total_count = None\n",
    "    batch_num = 1\n",
    "    \n",
    "    while True:\n",
    "        print(f\"   Fetching batch {batch_num} (offset: {offset})...\", end=\" \")\n",
    "        df_batch, total_count, has_more = fetch_data(offset=offset, length=length)\n",
    "        all_dataframes.append(df_batch)\n",
    "        print(f\"‚úÖ Got {len(df_batch)} records\")\n",
    "        \n",
    "        if not has_more:\n",
    "            break\n",
    "        \n",
    "        offset += length\n",
    "        batch_num += 1\n",
    "    \n",
    "    # Combine all batches\n",
    "    df_all = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Total records fetched: {len(df_all):,}\")\n",
    "    print(f\"   Expected total: {total_count:,}\")\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch ALL Data\n",
    "\n",
    "This will fetch all available data from the EIA API. It may take a few minutes depending on data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fetching data from EIA API...\n",
      "   Fetching batch 1 (offset: 0)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 2 (offset: 5000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 3 (offset: 10000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 4 (offset: 15000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 5 (offset: 20000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 6 (offset: 25000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 7 (offset: 30000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 8 (offset: 35000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 9 (offset: 40000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 10 (offset: 45000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 11 (offset: 50000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 12 (offset: 55000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 13 (offset: 60000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 14 (offset: 65000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 15 (offset: 70000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 16 (offset: 75000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 17 (offset: 80000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 18 (offset: 85000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 19 (offset: 90000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 20 (offset: 95000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 21 (offset: 100000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 22 (offset: 105000)... ‚úÖ Got 5000 records\n",
      "   Fetching batch 23 (offset: 110000)... ‚úÖ Got 112 records\n",
      "\n",
      "‚úÖ Total records fetched: 110,112\n",
      "   Expected total: 110,112\n",
      "\n",
      "‚úÖ Data loaded successfully! Shape: (110112, 13)\n"
     ]
    }
   ],
   "source": [
    "# Fetch ALL data (handles pagination automatically)\n",
    "try:\n",
    "    df_raw = fetch_all_data()\n",
    "    print(f\"\\n‚úÖ Data loaded successfully! Shape: {df_raw.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching data: {e}\")\n",
    "    print(\"   Please check your API key and internet connection, then re-run this cell.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PHASE 2: Data Exploration\n",
    "\n",
    "## Step 3: Initial Data Exploration\n",
    "\n",
    "Let's explore the raw data to understand its structure and identify preprocessing needs.\n",
    "\n",
    "**Note:** Make sure the data fetching cell completed successfully before running the exploration cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using df_raw from memory\n",
      "============================================================\n",
      "üìä RAW DATA OVERVIEW\n",
      "============================================================\n",
      "\n",
      "üìè Shape: 110,112 rows √ó 13 columns\n",
      "\n",
      "üìÖ Date Range:\n",
      "   First period: 2001-01\n",
      "   Last period: 2025-08\n",
      "\n",
      "üèõÔ∏è  States: 62 unique states\n",
      "   ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'ENC', 'ESC', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MAT', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'MTN', 'NC', 'ND', 'NE', 'NEW', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PACC', 'PACN', 'RI', 'SAT', 'SC', 'SD', 'TN', 'TX', 'US', 'UT', 'VA', 'VT', 'WA', 'WI', 'WNC', 'WSC', 'WV', 'WY']\n",
      "\n",
      "üè≠ Sectors: 6 unique sectors\n",
      "   ['ALL', 'COM', 'IND', 'OTH', 'RES', 'TRA']\n",
      "\n",
      "============================================================\n",
      "üìã COLUMN INFORMATION\n",
      "============================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110112 entries, 0 to 110111\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   period            110112 non-null  object\n",
      " 1   stateid           110112 non-null  object\n",
      " 2   stateDescription  110112 non-null  object\n",
      " 3   sectorid          110112 non-null  object\n",
      " 4   sectorName        110112 non-null  object\n",
      " 5   customers         65720 non-null   object\n",
      " 6   price             91760 non-null   object\n",
      " 7   revenue           91760 non-null   object\n",
      " 8   sales             91760 non-null   object\n",
      " 9   customers-units   110112 non-null  object\n",
      " 10  price-units       110112 non-null  object\n",
      " 11  revenue-units     110112 non-null  object\n",
      " 12  sales-units       110112 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 10.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check if df_raw exists, if not try to load from saved file\n",
    "try:\n",
    "    # Try to access df_raw\n",
    "    _ = df_raw.shape\n",
    "    print(\"‚úÖ Using df_raw from memory\")\n",
    "except (NameError, AttributeError):\n",
    "    print(\"‚ö†Ô∏è  df_raw not found. Attempting to load from saved file...\")\n",
    "    \n",
    "    # Try to find the most recent raw data file\n",
    "    data_raw_dir = project_root / 'data' / 'raw'\n",
    "    if data_raw_dir.exists():\n",
    "        csv_files = list(data_raw_dir.glob('eia_retail_sales_raw_*.csv'))\n",
    "        if csv_files:\n",
    "            # Get the most recent file\n",
    "            latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
    "            print(f\"üìÇ Loading from: {latest_file.name}\")\n",
    "            df_raw = pd.read_csv(latest_file)\n",
    "            print(f\"‚úÖ Loaded {len(df_raw):,} records from saved file\")\n",
    "        else:\n",
    "            print(\"‚ùå No saved data files found.\")\n",
    "            print(\"   Please run the data fetching cell above first.\")\n",
    "            raise NameError(\"df_raw is not defined. Please fetch data first.\")\n",
    "    else:\n",
    "        print(\"‚ùå Data directory not found.\")\n",
    "        print(\"   Please run the data fetching cell above first.\")\n",
    "        raise NameError(\"df_raw is not defined. Please fetch data first.\")\n",
    "\n",
    "# Basic data overview\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä RAW DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìè Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
    "print(f\"\\nüìÖ Date Range:\")\n",
    "print(f\"   First period: {df_raw['period'].min()}\")\n",
    "print(f\"   Last period: {df_raw['period'].max()}\")\n",
    "\n",
    "print(f\"\\nüèõÔ∏è  States: {df_raw['stateid'].nunique()} unique states\")\n",
    "print(f\"   {sorted(df_raw['stateid'].unique())}\")\n",
    "\n",
    "print(f\"\\nüè≠ Sectors: {df_raw['sectorid'].nunique()} unique sectors\")\n",
    "print(f\"   {sorted(df_raw['sectorid'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã COLUMN INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(df_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üëÄ FIRST 10 ROWS\n",
      "============================================================\n",
      "    period stateid stateDescription sectorid      sectorName customers  price  \\\n",
      "0  2025-08      AK           Alaska      ALL     all sectors    362846  24.02   \n",
      "1  2025-08      AK           Alaska      COM      commercial     58428  23.01   \n",
      "2  2025-08      AK           Alaska      IND      industrial      1244  21.26   \n",
      "3  2025-08      AK           Alaska      OTH           other      None   None   \n",
      "4  2025-08      AK           Alaska      RES     residential    303174  27.71   \n",
      "5  2025-08      AK           Alaska      TRA  transportation         0      0   \n",
      "6  2025-08      AL          Alabama      ALL     all sectors   2843428  12.79   \n",
      "7  2025-08      AL          Alabama      COM      commercial    395483   14.4   \n",
      "8  2025-08      AL          Alabama      IND      industrial      7261   7.72   \n",
      "9  2025-08      AL          Alabama      OTH           other      None   None   \n",
      "\n",
      "      revenue       sales      customers-units              price-units  \\\n",
      "0   113.03779   470.53238  number of customers  cents per kilowatt-hour   \n",
      "1    48.60317   211.21155  number of customers  cents per kilowatt-hour   \n",
      "2    24.43631   114.95066  number of customers  cents per kilowatt-hour   \n",
      "3        None        None  number of customers  cents per kilowatt-hour   \n",
      "4    39.99831   144.37018  number of customers  cents per kilowatt-hour   \n",
      "5           0           0  number of customers  cents per kilowatt-hour   \n",
      "6  1080.46794  8445.35498  number of customers  cents per kilowatt-hour   \n",
      "7   318.71819  2212.88312  number of customers  cents per kilowatt-hour   \n",
      "8   225.35729  2920.14424  number of customers  cents per kilowatt-hour   \n",
      "9        None        None  number of customers  cents per kilowatt-hour   \n",
      "\n",
      "     revenue-units             sales-units  \n",
      "0  million dollars  million kilowatt hours  \n",
      "1  million dollars  million kilowatt hours  \n",
      "2  million dollars  million kilowatt hours  \n",
      "3  million dollars  million kilowatt hours  \n",
      "4  million dollars  million kilowatt hours  \n",
      "5  million dollars  million kilowatt hours  \n",
      "6  million dollars  million kilowatt hours  \n",
      "7  million dollars  million kilowatt hours  \n",
      "8  million dollars  million kilowatt hours  \n",
      "9  million dollars  million kilowatt hours  \n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"=\" * 60)\n",
    "print(\"üëÄ FIRST 10 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "print(df_raw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "           Missing Count  Missing Percentage\n",
      "customers          44392           40.315315\n",
      "price              18352           16.666667\n",
      "revenue            18352           16.666667\n",
      "sales              18352           16.666667\n",
      "\n",
      "‚ö†Ô∏è  4 columns have missing values\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "missing = df_raw.isnull().sum()\n",
    "missing_pct = (missing / len(df_raw)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "if len(missing_df) == 0:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(missing_df)} columns have missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üî¢ DATA TYPES & SAMPLE VALUES\n",
      "============================================================\n",
      "\n",
      "üìå period:\n",
      "   Type: object\n",
      "   Unique values: 296\n",
      "   Sample values: ['2025-08', '2025-07', '2025-06', '2025-05', '2025-04']\n",
      "\n",
      "üìå stateid:\n",
      "   Type: object\n",
      "   Unique values: 62\n",
      "   Sample values: ['AK', 'AL', 'AR', 'AZ', 'CA']\n",
      "\n",
      "üìå stateDescription:\n",
      "   Type: object\n",
      "   Unique values: 62\n",
      "   Sample values: ['Alaska', 'Alabama', 'Arkansas', 'Arizona', 'California']\n",
      "\n",
      "üìå sectorid:\n",
      "   Type: object\n",
      "   Unique values: 6\n",
      "   Sample values: ['ALL', 'COM', 'IND', 'OTH', 'RES']\n",
      "\n",
      "üìå sectorName:\n",
      "   Type: object\n",
      "   Unique values: 6\n",
      "   Sample values: ['all sectors', 'commercial', 'industrial', 'other', 'residential']\n",
      "\n",
      "üìå customers:\n",
      "   Type: object\n",
      "   Unique values: 49263\n",
      "   Sample values: ['362846', '58428', '1244', '303174', '0']\n",
      "\n",
      "üìå price:\n",
      "   Type: object\n",
      "   Unique values: 3100\n",
      "   Sample values: ['24.02', '23.01', '21.26', '27.71', '0']\n",
      "\n",
      "üìå revenue:\n",
      "   Type: object\n",
      "   Unique values: 84087\n",
      "   Sample values: ['113.03779', '48.60317', '24.43631', '39.99831', '0']\n",
      "\n",
      "üìå sales:\n",
      "   Type: object\n",
      "   Unique values: 84716\n",
      "   Sample values: ['470.53238', '211.21155', '114.95066', '144.37018', '0']\n",
      "\n",
      "üìå customers-units:\n",
      "   Type: object\n",
      "   Unique values: 1\n",
      "   Sample values: ['number of customers']\n",
      "\n",
      "üìå price-units:\n",
      "   Type: object\n",
      "   Unique values: 1\n",
      "   Sample values: ['cents per kilowatt-hour']\n",
      "\n",
      "üìå revenue-units:\n",
      "   Type: object\n",
      "   Unique values: 1\n",
      "   Sample values: ['million dollars']\n",
      "\n",
      "üìå sales-units:\n",
      "   Type: object\n",
      "   Unique values: 1\n",
      "   Sample values: ['million kilowatt hours']\n"
     ]
    }
   ],
   "source": [
    "# Check data types and sample values\n",
    "print(\"=\" * 60)\n",
    "print(\"üî¢ DATA TYPES & SAMPLE VALUES\")\n",
    "print(\"=\" * 60)\n",
    "for col in df_raw.columns:\n",
    "    print(f\"\\nüìå {col}:\")\n",
    "    print(f\"   Type: {df_raw[col].dtype}\")\n",
    "    print(f\"   Unique values: {df_raw[col].nunique()}\")\n",
    "    if df_raw[col].dtype == 'object':\n",
    "        sample_values = df_raw[col].dropna().unique()[:5]\n",
    "        print(f\"   Sample values: {list(sample_values)}\")\n",
    "    else:\n",
    "        print(f\"   Min: {df_raw[col].min()}, Max: {df_raw[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìà STATISTICAL SUMMARY\n",
      "============================================================\n",
      "          customers         price       revenue          sales\n",
      "count  6.572000e+04  91760.000000  91760.000000   91760.000000\n",
      "mean   2.939946e+06      9.518629    604.270447    6014.848993\n",
      "std    1.210875e+07      5.231911   2238.732942   21442.701377\n",
      "min    0.000000e+00      0.000000     -0.000010       0.000000\n",
      "25%    4.938000e+03      6.730000     30.088248     288.586380\n",
      "50%    3.027645e+05      8.980000    124.417840    1451.580415\n",
      "75%    2.045894e+06     11.650000    432.670640    4360.588210\n",
      "max    1.652490e+08    116.670000  58596.122470  407230.479880\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary for numeric columns\n",
    "print(\"=\" * 60)\n",
    "print(\"üìà STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert numeric columns (they might be stored as strings)\n",
    "numeric_cols = ['customers', 'price', 'revenue', 'sales']\n",
    "for col in numeric_cols:\n",
    "    if col in df_raw.columns:\n",
    "        # Convert to numeric, handling 'None' strings\n",
    "        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
    "\n",
    "print(df_raw[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PHASE 3: Save Raw Data\n",
    "\n",
    "## Step 4: Save Raw Data\n",
    "\n",
    "Save the raw data to `data/raw/` for later preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving raw data to: data/raw/eia_retail_sales_raw_20251119_235002.csv\n",
      "‚úÖ Raw data saved successfully!\n",
      "   File size: 15.35 MB\n",
      "   Records: 110,112\n",
      "\n",
      "üíæ Also saved as Parquet: data/raw/eia_retail_sales_raw_20251119_235002.parquet\n",
      "   File size: 2.21 MB\n",
      "\n",
      "üìÅ Raw data file: eia_retail_sales_raw_20251119_235002.csv\n"
     ]
    }
   ],
   "source": [
    "# Create data directories if they don't exist\n",
    "data_raw_dir = project_root / 'data' / 'raw'\n",
    "data_raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"eia_retail_sales_raw_{timestamp}.csv\"\n",
    "filepath = data_raw_dir / filename\n",
    "\n",
    "# Save raw data as CSV\n",
    "print(f\"üíæ Saving raw data to: {filepath}\")\n",
    "df_raw.to_csv(filepath, index=False)\n",
    "print(f\"‚úÖ Raw data saved successfully!\")\n",
    "print(f\"   File size: {filepath.stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"   Records: {len(df_raw):,}\")\n",
    "\n",
    "# Also save as parquet for better performance (optional)\n",
    "try:\n",
    "    parquet_filename = f\"eia_retail_sales_raw_{timestamp}.parquet\"\n",
    "    parquet_filepath = data_raw_dir / parquet_filename\n",
    "    df_raw.to_parquet(parquet_filepath, index=False)\n",
    "    print(f\"\\nüíæ Also saved as Parquet: {parquet_filepath}\")\n",
    "    print(f\"   File size: {parquet_filepath.stat().st_size / (1024*1024):.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not save as Parquet: {e}\")\n",
    "    print(\"   CSV file saved successfully - Parquet is optional\")\n",
    "\n",
    "# Store the filepath for reference\n",
    "print(f\"\\nüìÅ Raw data file: {filepath.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• PHASE 4: Identify Preprocessing Needs\n",
    "\n",
    "## Step 5: Document Preprocessing Requirements\n",
    "\n",
    "Based on the exploration above, document what preprocessing steps will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß PREPROCESSING REQUIREMENTS IDENTIFIED\n",
      "============================================================\n",
      "‚úÖ Convert 'period' column to datetime\n",
      "   - Format appears to be YYYY-MM\n",
      "‚úÖ Handle missing values in 4 columns\n",
      "   - customers: 44,392 (40.3%)\n",
      "   - price: 18,352 (16.7%)\n",
      "   - revenue: 18,352 (16.7%)\n",
      "   - sales: 18,352 (16.7%)\n",
      "‚úÖ Ensure categorical columns are properly typed\n",
      "   - Columns: stateid, stateDescription, sectorid, sectorName\n",
      "‚úÖ Standardize units (may need to extract to separate columns or document)\n",
      "‚úÖ Add data validation checks\n",
      "   - Check for negative values in numeric columns\n",
      "   - Validate date ranges\n",
      "   - Check for outliers\n",
      "‚úÖ Potential feature engineering:\n",
      "   - Extract year and month from period\n",
      "   - Calculate derived metrics (e.g., revenue per customer)\n",
      "   - Create time-based features for forecasting\n",
      "\n",
      "============================================================\n",
      "üìù Next Steps:\n",
      "   1. Review the raw data structure above\n",
      "   2. Implement preprocessing pipeline based on identified needs\n",
      "   3. Save preprocessed data to data/processed/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of preprocessing needs\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß PREPROCESSING REQUIREMENTS IDENTIFIED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "preprocessing_needs = []\n",
    "\n",
    "# 1. Data type conversions\n",
    "if df_raw['customers'].dtype == 'object' or df_raw['price'].dtype == 'object':\n",
    "    preprocessing_needs.append(\"‚úÖ Convert numeric columns (customers, price, revenue, sales) from string to numeric\")\n",
    "    preprocessing_needs.append(\"   - Handle 'None' strings as NaN\")\n",
    "    preprocessing_needs.append(\"   - Convert to float64 for calculations\")\n",
    "\n",
    "# 2. Date handling\n",
    "if df_raw['period'].dtype == 'object':\n",
    "    preprocessing_needs.append(\"‚úÖ Convert 'period' column to datetime\")\n",
    "    preprocessing_needs.append(\"   - Format appears to be YYYY-MM\")\n",
    "\n",
    "# 3. Missing values\n",
    "missing_cols = df_raw.isnull().sum()\n",
    "if missing_cols.sum() > 0:\n",
    "    preprocessing_needs.append(f\"‚úÖ Handle missing values in {len(missing_cols[missing_cols > 0])} columns\")\n",
    "    for col in missing_cols[missing_cols > 0].index:\n",
    "        pct = (missing_cols[col] / len(df_raw)) * 100\n",
    "        preprocessing_needs.append(f\"   - {col}: {missing_cols[col]:,} ({pct:.1f}%)\")\n",
    "\n",
    "# 4. Categorical encoding\n",
    "categorical_cols = ['stateid', 'stateDescription', 'sectorid', 'sectorName']\n",
    "preprocessing_needs.append(\"‚úÖ Ensure categorical columns are properly typed\")\n",
    "preprocessing_needs.append(f\"   - Columns: {', '.join(categorical_cols)}\")\n",
    "\n",
    "# 5. Units standardization\n",
    "if 'customers-units' in df_raw.columns:\n",
    "    preprocessing_needs.append(\"‚úÖ Standardize units (may need to extract to separate columns or document)\")\n",
    "\n",
    "# 6. Data validation\n",
    "preprocessing_needs.append(\"‚úÖ Add data validation checks\")\n",
    "preprocessing_needs.append(\"   - Check for negative values in numeric columns\")\n",
    "preprocessing_needs.append(\"   - Validate date ranges\")\n",
    "preprocessing_needs.append(\"   - Check for outliers\")\n",
    "\n",
    "# 7. Feature engineering opportunities\n",
    "preprocessing_needs.append(\"‚úÖ Potential feature engineering:\")\n",
    "preprocessing_needs.append(\"   - Extract year and month from period\")\n",
    "preprocessing_needs.append(\"   - Calculate derived metrics (e.g., revenue per customer)\")\n",
    "preprocessing_needs.append(\"   - Create time-based features for forecasting\")\n",
    "\n",
    "print(\"\\n\".join(preprocessing_needs))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù Next Steps:\")\n",
    "print(\"   1. Review the raw data structure above\")\n",
    "print(\"   2. Implement preprocessing pipeline based on identified needs\")\n",
    "print(\"   3. Save preprocessed data to data/processed/\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Electricity Consumption)",
   "language": "python",
   "name": "electricity-consumption"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
